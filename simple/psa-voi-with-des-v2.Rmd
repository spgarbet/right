---
title: "Simple Event Model Value of Information"
output:
  html_document: default
  html_notebook: default
---

The objective of this notebook is to explore probabilistic sensitivity analysis with a discrete event simulation model with the following parameters:

1. A population of 40-year old women are at risk for both secular death and an event (A) that happens at a **10% [8-12%]** rate over a 5 year period.
2. All those who experience the event incur a cost of $10,000.
3. Among those who experience the event, there is a **5% [2-7%]** case fatality rate.  
4. Among those who survive, they experience a **0.25 [0.2-0.3]** utility decrement for the remainder of their life. 
5. Among the surviors, there is a second event (B) that occurs with probability **50% [30-70]** over a 5 year period.
6. Event B has no case fatality but incurs a $25,000 cost and a **0.10 [0.7-0.13]** disutility for 1 year 
7. There is a new therapy available that reduces the rate of the first event by a relative risk of **0.5 [0.45-0.55]**, however the therapy costs $8,000.

This model was intially coded up as a DES and then a numerical solution was derived using a delay differential equation. 


# Numerical Solution to Model
```{r,include=FALSE}
require(tidyverse)
library(deSolve)
library(flexsurv) # For pgompertz
library(scales)
require(plyr)
require(reshape2)
require(ellipse)
source("./ce-psa-functions.r")
ss_death <- read.csv("ss-death-2011.csv")

inst_rate <- function(percent, timeframe)
{
  - log(1-percent) / timeframe
}

###################################
# Main model parameters
params <- c(
  r_a  = inst_rate(0.1, 5),  # Rate of healthy having event A
  r_b  = inst_rate(0.5, 5),  # Rate of post-A  having event B
  r_ad = 0.05,               # Rate of death as direct result of A
  
  c_a  = 15000,              # Cost of event A
  c_b  = 15000,              # Cost of event B for a year
  c_t  = 0,                  # Cost of therapy
  d_a  = 0.25,               # Permanent disutility for a
  d_b  = 0.1,                # 1-year disutility for b 
  
  disc_rate = 1e-12          # For computing discount
)


###################################
# Numerical approach to secular death (very high accuracy!)
f_40yr_percent_d    <- c(ss_death$f_death_prob[41:120])
sim_adj_age         <- 0:79 + 0.5 # 0.5 offset since percentage is for whole year
f_40yr_per_d_spline <- splinefun(sim_adj_age, f_40yr_percent_d)
#plot(1:2); dev.off()
#curve(f_40yr_per_d_spline, col='red', from=0, to=82, xlab="years past 40", ylab="percent chance of death")
#points(sim_adj_age, f_40yr_percent_d)

# Clamped at infinite rate via pmin
f_40yr_drate <- function(t) inst_rate(pmin(f_40yr_per_d_spline(t), 1),1)
#curve(f_40yr_drate, from=0, to=90)

####################################
# Integrations of death rates for exposure calculations in delay
# Now, a special function used in delay equation (Had to put upper bound at 81)
F_40yr_drate_5yr <- Vectorize(function(t)
{
  integrate(f_40yr_drate, lower=max(t-5, 0), upper=min(t, 81))$value
})


F_40yr_drate_1yr <- Vectorize(function(t)
{
  integrate(f_40yr_drate, lower=max(t-1, 0), upper=min(t, 81))$value
})


# Does a spline work faster?

x <- 0:160 / 2
y <- exp(-5*params['r_b'] - F_40yr_drate_5yr(x))
#plot(x, y, typ='l')
f <- splinefun(x, y)
#curve(f, add=TRUE, col='red', lty=2)
F_40yr_drate_5yr <- f


x <- 0:160 / 2
y <- exp(-F_40yr_drate_1yr(x))
#plot(x, y, typ='l')
f <- splinefun(x, y)
#curve(f, add=TRUE, col='red', lty=2)
F_40yr_drate_1yr <- f

# This is for doing numberical integration of a set of numbers at an even interval
alt_simp_coef <- function(i)
{
  if (i < 8) stop("Invalid Simpson coefficient size")
  
  # pg.117 4.1.14, Numerical Recipes in C, 1st edition
  c(17/48, 50/48, 43/48, 49/48, rep(1, i-8), 49/48, 43/48, 50/48, 17/48) 
}

###################################
# Numerical Delay Differential Equation
Simple <- function(t, y, params)
{
  with(as.list(c(y, params)), {
    
    # Use table for death_prob, Female 40 (offset 1)
    r_d <- f_40yr_drate(t)
    if(is.infinite(r_d)) r_d <- 1e16 # A really large number

    # Event B stops at time 5 years after event A (delay equation)
    dd_b <- if (t < 5 || t> 10) 0 else (1-r_ad)*r_a*lagvalue(t-5, 2)*F_40yr_drate_5yr(t)

    # Event A stops at time t=5 years
    if(t > 5) r_a <- 0
      
    list(c(
            disc = -disc_rate*disc,            # Simple discount r ate
            h    = -(r_a+r_d)*h,
            a    = r_a*h,
            e10  = (1-r_ad)*r_a*h-r_d*e10-dd_b-r_b*e10,
            e15  = dd_b - r_d*e15,
            b    = r_b*e10,
            e2   = r_b*e10 - r_d*e2,
            d    = r_ad*r_a*h + r_d*(h+e10+e15+e2),
            db   = r_b*e10 - r_d*db - if (t < 1 || t > 11) 0 else lagderiv(t-1, 6)*F_40yr_drate_1yr(t)
          )
    )
  })
}

yinit <- c(disc=1, h=1, a=0, e10=0, e15=0, b=0, e2=0, d=0, db=0)
times <- seq(0, 40, by=1/365)  # units of years, increments of days, everyone dies after 120, so simulation is cut short
#system.time(out <- dede(yinit, times, Simple, params)) #, control=list(mxhist=1e6)))
#out   <- dede(yinit, times, Simple, params, control=list(mxhist=1e6))

costs <- function(solution, params)
{
  n <- length(solution[,1])
  simpson <- alt_simp_coef(n)

  with(as.list(params), {
    # Compute Discounted Cost
    cost <- c_a*sum(diff(solution[,'a'])*solution[2:n,'disc']) + # Cost * Number of events in bucket a
            c_b*sum(diff(solution[,'b'])*solution[2:n,'disc']) + # Cost * Number of events in bucket b
            c_t*solution[1, 'h']  # Therapy Cost * Initial healthy individuals
    
    # Step size of simulation
    step     <- solution[2,'time'] - solution[1,'time']
    
    # Total possible life units is integral of discounted time
    life <- sum(simpson*solution[,'disc'])*step
    
    # Permanent disutility for A (integration)
    disA <- d_a*sum(simpson*solution[,'e10']*solution[,'disc'])*step + 
            d_a*sum(simpson*solution[,'e15']*solution[,'disc'])*step +
            d_a*sum(simpson*solution[,'e2' ]*solution[,'disc'])*step
    
    # Event B
    disB <- d_b*sum(simpson*solution[,'db']*solution[,'disc'])*step

    # Death disutility
    disD <- sum(simpson*solution[,'d']*solution[,'disc'])*step       # Death disutility (integration)
    
    c(cost       = unname(cost),
      qaly       = unname(life-disA-disB-disD),
      possible   = unname(life),
      disutility = unname(disA+disB+disD),
      a_count    = unname(solution[n,'a']),
      disutil_a  = unname(disA),
      b_count    = unname(solution[n,'b']),
      disutil_b  = unname(disB),
      dead_count = unname(solution[n,'d']), 
      disutil_d  = unname(disD),
      living     = unname(solution[n,'h']+solution[n,'e10']+solution[n,'e15']+solution[n,'e2'])
      )
  })
}
expected <- function(params) costs(dede(yinit, times, Simple, params), params)
```

# Single Model Run
```{r,cache=TRUE}
lambda <- 50000

# Baseline Run
params['r_a'] <- params['r_a']
params['c_t']  <- 0
A <- as.data.frame(expected(params))
NMB.A <- lambda*A["qaly",] - A["cost",]; NMB.A
  
# With Treatment
params['r_a'] <- params['r_a']*0.5
params['c_t']  <- 10000
B <- as.data.frame(expected(params))
NMB.B <- lambda*B["qaly",] - B["cost",]; NMB.B

(ICER = (B["cost",]- A["cost",]) / (B["qaly",]-A["qaly",]))

```

# Probabilistic Sensitivity Analysis
```{r}
# Parameter List
params.psa <- list(
  r_a = list( # Rate of healthy having event A [uinform(8,12)]
    value = inst_rate(0.1, 5),
    type = "uniform",
    parameter = "r_a",
    min = 0.08,
    max = 0.10
  ),
  r_b = list( # Rate of post-A  having event B [unifirm(30-70)]
    value = inst_rate(0.5, 5),
    type = "uniform",
    parameter = "r_a",
    min = 0.30,
    max = 0.70
    ),
  r_ad = list( # Rate of death as direct result of A [beta(50,950)]
    value = 0.05,
    type = "beta",
    parameter = "r_ad",
    shape1 = 50,
    shape2 = 950
    ),
  c_a = list( # Cost of event A
    value = 15000,
    type = "uniform",
    parameter = "c_a",
    min = 15000,
    max = 15000
    ),  
  c_b = list( # Cost of event B
    value = 15000,
    type = "uniform",
    parameter = "c_b",
    min = 15000,
    max = 15000
    ), 
  c_t = list( # Cost of therapy
    value = 10000,
    type = "uniform",
    parameter = "c_b",
    min = 10000,
    max = 10000
    ),
  d_a = list( # Permanent disutility for a [beta(25,75)]
    value = 0.25,
    type = "beta",
    parameter = "d_a",
    shape1 = 25,
    shape2 = 75  
    ),
  d_b = list(    # 1-year disutility for b  [beta(100,900)]
    value = 0.10,
    type = "beta",
    parameter = "d_b",
    shape1 = 100,
    shape2 = 900
  ),
  rr_t = list( #RR of reducing A = 0.5 [beta(50,50)]
    value = 1,
    type = "beta",
    parameter = "rr_t",
    shape1 = 50,
    shape2 = 50             
  ),
  disc_rate = list( # Discount Rate
    value = 1e-12,
    type = "uniform",
    parameter = "disc_rate",
    min = 1e-12,
    max = 1e-12
    )
)

params <- unlist(lapply(params.psa,function(x) x$value))

```
We'll now draw a random hypercube sample based on the number of parameters listed in the list object.  Once we draw the uniform draws from the hypercube sample, we'll apply the specific quantile function for the model parameter to get its draw for that iteration of the model.

```{r}
PSA.N = 4000
require(lhs)
require(tidyverse)
X <- randomLHS(PSA.N, length(params.psa))
colnames(X) = names(params.psa)

lhc.draws.transformed <- cbind.data.frame(lapply(params.psa,function(x) 
  {
    if (x[["type"]]=="beta")
    {
      qbeta(X[,x[["parameter"]]],shape1=x[["shape1"]],shape2=x[["shape2"]])
    } 
    else if (x[["type"]]=="uniform")
    {
      qunif(X[,x[["parameter"]]],min=x[["min"]],max=x[["max"]])
    }   
  }
))
lhc.draws.transformed %>% head()

```


```{r,cache=TRUE}
# Set up Parallel Processing
# require(doParallel)
# nworkers <- detectCores()
# cl <- makePSOCKcluster(nworkers)
# clusterSetRNGStream(cl, c(1,2,3,4,5,6,7,8))
# registerDoParallel(cl)
# 
# # Split Into Chunks
# d = 1:dim(lhc.draws.transformed)[1]
# chunks = split(d,ceiling(seq_along(d)/100))

if (!file.exists("./psa-toy.Rdata"))
{
  PSA <- foreach (i = 1:dim(lhc.draws.transformed)[1],.combine=rbind) %do%
  {
    cat(i)
    # Standard of Care
    params = lhc.draws.transformed[i,]
    params$r_a = inst_rate(params$r_a, 5)
    params$r_b = inst_rate(params$r_b, 5)
    params['rr_t'] = 1
    params['r_a'] <- params['r_a']*params['rr_t']
    params['c_t']  <- 0
    A <- as.data.frame(expected(params))
    params.A <- params
    names(params.A) = paste0(names(params.A),".A")
    
    # Treatment
    params = lhc.draws.transformed[i,]
    params$r_a = inst_rate(params$r_a, 5)
    params$r_b = inst_rate(params$r_b, 5)
    params['r_a'] <- params['r_a']*params['rr_t']
    B <- as.data.frame(expected(params))
    params.B <- params
    names(params.B) = paste0(names(params.B),".B")
    
    
    out <- unlist(c(params.A,
                    params.B, 
                    cost.A = A["cost",] , qaly.A = A["qaly",] , Strategy.A = lambda*A["qaly",] - A["cost",],
                    cost.B = B["cost",] , qaly.B = B["qaly",] , Strategy.B = lambda*B["qaly",] - B["cost",]))
    out
  }
  
  PSA <- PSA %>% tbl_df()
  save(PSA,file="./psa-toy.Rdata")
} else
{
  load("./psa-toy.Rdata")
}


```


# Probalisitc Sensitivity Analysis
```{r}
load("./psa-toy.Rdata")
select <- dplyr::select
mutate <- dplyr::mutate
Sim <- PSA %>% tbl_df()
Sim <- PSA %>% mutate(iteration=row_number()) %>% select(iteration,cost.A,qaly.A,cost.B,qaly.B,contains(".B"))
Sim2 <- read.csv("../../VOI_SMDM_2016/Metamodeling_SensitivityAnalysis_SMDM2014/PSA.csv")

Strategies<-c("Standard", "Therapy")

#Determine the number of strategies
ndep<-length(Strategies)

#Create vector of variable names
Names <- colnames(Sim)
Parms <- Sim %>% select(-iteration,-cost.A,-cost.B,-qaly.A,-qaly.B,-Strategy.B,-disc_rate.B) %>% data.frame() #Sim2[,(ndep*2+2):dim(Sim2)[2]]
#Get parameter names
paramNames<-colnames(Parms)
indep<-ncol(Parms)
Outcomes <- Sim %>%  select(Standard_cost=cost.A,Standard_eff=qaly.A,Therapy_cost=cost.B,Therapy_eff=qaly.B) %>% data.frame

```

In the simulation file we only have each parameter's sample and the resulting cost and effectiveness for each of the strategies for each set of the parameters' realization. We need to construct the **Net Health Benefit (NHB)** for each of the strategies. The NHB for strategy $k$ is defined as 
$$ \bar{NHB}_k = \bar{E}_k - \bar{C}_k/\lambda ,$$
where $\bar{E}_k$ and $\bar{C}_k$ are the expected effectiveness and cost for strategy $k$, respectively; and $\lambda$ is the willingness-to-pay threshold.


```{r}
  lambda <- 50000
```

## Calculate NHB For Each Strategy

```{r}
  Sim$A_NHB <- Sim$qaly.A- Sim$cost.A/lambda
  Sim$B_NHB <- Sim$qaly.B- Sim$cost.B/lambda
  #Save the NHB for each strategy into a new data frame
  NHB <- Sim %>% select(A_NHB,B_NHB)
```

# Linear Metamodel


```{r}

# Identify the parameters that vary
paramvar <- Sim[,paramNames] %>% summarise_all(function(x) var(x))
paramNames.varying <- names(paramvar)[which(paramvar!=0)]

## Cost ##
YY = "cost.A"
fmla.unstd <- as.formula(paste0(YY,"~",paste0(paramNames.varying,collapse="+")))
fmla.std <- as.formula(paste0(YY,"~",paste0(paste0("scale(",paramNames.varying,")"),collapse="+")))
A.cost.fit <- lm(fmla.unstd, data=Sim)
A.cost.fit.std <- lm(fmla.std,data=Sim)

## Effectiveness ##
YY = "qaly.A"
fmla.unstd <- as.formula(paste0(YY,"~",paste0(paramNames.varying,collapse="+")))
fmla.std <- as.formula(paste0(YY,"~",paste0(paste0("scale(",paramNames.varying,")"),collapse="+")))
A.eff.fit <- lm(fmla.unstd, data=Sim)
A.eff.fit.std <- lm(fmla.std,data=Sim)

## NHB ##
YY = "A_NHB"
fmla.unstd <- as.formula(paste0(YY,"~",paste0(paramNames.varying,collapse="+")))
fmla.std <- as.formula(paste0(YY,"~",paste0(paste0("scale(",paramNames.varying,")"),collapse="+")))
A.nhb.fit <- lm(fmla.unstd, data=Sim)
A.nhb.fit.std <- lm(fmla.std,data=Sim)


```

```{r,results='asis'}
stargazer::stargazer(A.cost.fit,A.eff.fit,A.nhb.fit,
          title="Regression Coefficients from Metamodels on the Outcomes of Chemotherapy",
          type='html',
          dep.var.caption=" With **unstandardized** predictors ",
          dep.var.labels=c("Cost","Effectiveness","NHB"),
          digits=2,
          digits.extra=1,
          keep.stat=c("rsq","n"),
          notes = c('NHB: Net health benefit  '),
          notes.align = 'l',
          notes.append = FALSE,
          omit.table.layout="#",
          report=c("vc"))
```  
  
  
  
```{r,results='asis'}
stargazer::stargazer(A.cost.fit.std,A.eff.fit.std,A.nhb.fit.std,
          title="Regression Coefficients from Metamodels on the Outcomes of Chemotherapy",
          type='html',
          dep.var.caption=" With **standardized** predictors ",
          dep.var.labels=c("Cost","Effectiveness","NHB"),
          digits=2,
          digits.extra=1,
          keep.stat=c("rsq","n"),
          notes = c('NHB: Net health benefit  '),
          notes.align = 'l',
          notes.append = FALSE,
          omit.table.layout="#",
          report=c("vc"))
```  


## One-Way Sensitivity Analysis



```{r, eval=FALSE}
Sim.Long <- Sim %>% tbl_df() %>% select(one_of(c("iteration",paramNames.varying,"A_NHB","B_NHB"))) %>% data.frame() %>% 
  melt(id.vars=c("iteration",paramNames.varying),measure.vars= c("A_NHB","B_NHB"))
```

Next we define the parameter for which we desire to do the one-way sensitivity analysis and its range. 
```{r}
parm<-'d_a.B'
```

The graph over the parameter's sample domain looks like
```{r Oneway-NoRange, fig.width=8, fig.height=6}
OneWaySA(Strategies,Parms,NHB,parm) 
```
# Tornado Diagram

```{r Tornado Opt, fig.width=7, fig.height=6,warning=FALSE}
NHB <- NHB %>% data.frame()
Parms <- Parms %>% data.frame()
TornadoOpt(Parms[,paramNames.varying],NHB)
```

```{r Tornado All, fig.width=7, fig.height=6,warning=FALSE, message=FALSE}
TornadoAll(Strategies,Parms[,paramNames.varying],NHB)
```

# Threshold Analysis

To perfom a threshold analysis we need to do a linear metamodel over all the different strategies. We will do it only for the standardized parameters. We will also include the interactions of the variables to perform a two-way sensitivity analysis in the next section.

# Cost Effectiveness Plane
```{r}
#PlaneCE(Strategies,Outcomes)
```


# Cost-Effectiveness Scatterplot

```{r CE-Scatter, fig.width=7, fig.height=6,warning=FALSE}
ScatterCE(Strategies,Outcomes)
```


```{r}
  lambda_range<-c(1000,150000,50)
```

```{r CEAC, fig.width=7, fig.height=6,warning=FALSE}
  CEAC(lambda_range,Strategies,Outcomes)
```


# Value of Information 

## Plot Net Monetary Benefit For Each Strategy

```{r}
lambda <- 50000
n.sim <- nrow(PSA)
nmb <-  PSA %>% mutate(Standard = qaly.A*lambda-cost.A, Therapy = qaly.B*lambda - cost.B) %>% select(Standard, Therapy) %>% data.frame()

# Number of Strategies
n.strategies <- ncol(nmb)
n.strategies

# Assign name of strategies
strategies <- c("Standard","Therapy")
colnames(nmb) <- strategies

## Format data frame suitably for plotting
require(reshape2)
nmb.gg <- melt(nmb,  
               variable.name = "Strategy", 
               value.name = "NMB")
```


```{r,results='asis'}
## Plot NMB for different strategies
require(ggplot2)
require(scales)  # For dollar labels
require(grid)
number_ticks <- function(n) {function(limits) pretty(limits, n)} 
# Faceted plot by Strategy
ggplot(nmb.gg, aes(x = NMB/1000)) +
  geom_histogram(aes(y =..density..), col="black", fill = "gray") +
  geom_density(color = "red") +
  facet_wrap(~ Strategy, scales = "free_y") +
  xlab("Net Monetary Benefit (NMB) x10^3") +
  scale_x_continuous(breaks = number_ticks(5), labels = dollar) + 
  scale_y_continuous(breaks = number_ticks(5)) + 
  theme_bw()
```

```{r}
#### Incremental NMB (INMB) ####
# Calculate INMB of B vs A
# Only B vs A but we could have plotted all combinations

inmb <- data.frame(Simulation = 1:n.sim,
                   `Therapy vs Standard` = nmb$`Therapy` - nmb$`Standard`) 

## Format data frame suitably for plotting
inmb.gg <- melt(inmb, id.vars = "Simulation", 
                variable.name = "Comparison", 
                value.name = "INMB")
txtsize<-16
```

```{r,results='asis'}
## Plot INMB
ggplot(inmb.gg, aes(x = INMB/1000)) +
  geom_histogram(aes(y =..density..), col="black", fill = "gray") +
  geom_density(color = "red") +
  geom_vline(xintercept = 0, col = 4, size = 1.5, linetype = "dashed") +
  facet_wrap(~ Comparison, scales = "free_y") +
  xlab("Incremental Net Monetary Benefit (INMB) in thousand $") +
  scale_x_continuous(breaks = number_ticks(5), limits = c(-100, 100)) + 
  scale_y_continuous(breaks = number_ticks(5)) + 
  theme_bw(base_size = 14)
```

```{r}
#### Loss Matrix ####  
# Find optimal strategy (d*) based on the highest expected NMB
d.star <- which.max(colMeans(nmb))
d.star

## Compute Loss matrix iterating over all strategies
# Initialize loss matrix of dimension: number of simulation by number of strategies
loss <- matrix(0, n.sim, n.strategies)
# Or without iterating (much faster!)
loss <- as.matrix(nmb - nmb[, d.star])
head(loss)

#### EVPI ####
## Find maximum loss overall strategies at each state of the world 
## (i.e., PSA sample)
require(matrixStats)
max.loss.i <- rowMaxs(loss)
head(max.loss.i)
## Average across all states of the world
evpi <- mean(max.loss.i)
evpi

```




